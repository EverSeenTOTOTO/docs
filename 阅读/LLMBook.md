# 大语言模型[^link]

[^link]: https://llmbook-zh.github.io/

1. 大语言模型主要通过预测下一个词元的预训练任务进行学习，虽然并没有针对特定的下游任务进行优化，却能够建立远强于传统模型的通用任务求解能力。实际上，<Notation>基于大规模无标注文本的下一个词元预测任务本质上可以看作一个多任务学习过程</Notation>，因为针对不同词元的预测任务可能涉及到情感分类（“...这部电影真好看”）、数值计算（“3+4=7”）、知识推理（“中国陆地面积最大的省份是新疆”）等非常多样的训练任务。

> 语言模型将每个（自然语言处理）任务都视为基于世界文本子集的下一个词预测问题。因此，如果无监督语言建模经过训练后具有足够的能力复原全部世界文本，那么本质上它就能够解决各种任务。

2. 大语言模型经过超大规模数据的预训练后，能够编码大量的文本语义知识信息。然而，这个阶段的模型能力仍然是通过通用的下一个词预测任务建立的，主要目的是为了进行预训练文本数据的恢复。为了提升模型的任务求解能力，需要设计合适的指令微调以及提示策略进行激发或诱导。在指令微调方面，可以使用自然语言表达的任务描述以及期望的任务输出对于大语言模型进行指令微调，从而增强大语言模型的通用任务求解能力，提升模型在未见任务上的泛化能力。通常来说，<Notation>现有的研究认为指令微调无法向大模型注入新的知识，而是训练大模型学会利用自身所掌握的知识与信息进行任务的求解</Notation>。

> 不要被“微调”这个词汇所迷惑，预训练之后的大模型并不是一个完成品仅需要作些小优化。这个阶段的模型并不能理解任务，假如你告诉它写一封邮件，它大概率只会接着生成一些邮件相关的字句。微调才是让其具备任务求解能力的关键。

> 摘自后文：一般来说，指令微调很难教会大语言模型预训练阶段没有学习到的知识与能力，它主要起到了对于模型能力的激发作用，而不是知识注入作用。

3. OpenAI 前首席科学家 Ilya Sutskever 在公开采访中指出大规模预训练本质上是在做一个世界知识的压缩，从而能够学习到一个编码世界知识的参数模型，这个模型能够通过解压缩所需要的知识来解决真实世界的任务。
